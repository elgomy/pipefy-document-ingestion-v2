#!/usr/bin/env python3
"""
Script de prueba completo para demostrar la integraciÃ³n automÃ¡tica
de mÃ©tricas con el sistema de manejo de errores.
"""

import asyncio
import time
import logging
import sys
from pathlib import Path

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

# Agregar src al path de Python
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

# Importar mÃ³dulos necesarios
from utils.error_handler import with_error_handling, RetryConfig, get_metrics_service
from services.metrics_service import ServiceType

# Funciones de prueba decoradas con error handling

@with_error_handling("pipefy", RetryConfig(max_retries=2, base_delay=0.1))
async def test_pipefy_success():
    """Simula una llamada exitosa a Pipefy."""
    await asyncio.sleep(0.1)  # Simular latencia
    return {"status": "success", "card_id": "12345"}

@with_error_handling("pipefy", RetryConfig(max_retries=2, base_delay=0.1))
async def test_pipefy_failure():
    """Simula una falla en Pipefy."""
    await asyncio.sleep(0.05)
    raise Exception("Pipefy API rate limit exceeded")

@with_error_handling("crewai", RetryConfig(max_retries=1, base_delay=0.2))
async def test_crewai_timeout():
    """Simula un timeout en CrewAI."""
    await asyncio.sleep(0.05)
    raise asyncio.TimeoutError("CrewAI service timeout")

@with_error_handling("twilio")  # Usar configuraciÃ³n por defecto
async def test_twilio_success():
    """Simula una llamada exitosa a Twilio."""
    await asyncio.sleep(0.15)
    return {"message_sid": "SM123456", "status": "sent"}

@with_error_handling("cnpj", RetryConfig(max_retries=1, base_delay=0.1))
def test_cnpj_sync_success():
    """Simula una llamada sÃ­ncrona exitosa a CNPJ."""
    time.sleep(0.12)
    return {"cnpj": "12345678000199", "company": "Test Company Ltd"}

@with_error_handling("supabase", RetryConfig(max_retries=1, base_delay=0.1))
async def test_supabase_failure():
    """Simula una falla en Supabase."""
    await asyncio.sleep(0.08)
    raise ConnectionError("Database connection lost")

async def run_comprehensive_test():
    """Ejecuta una prueba completa del sistema integrado."""
    logger.info("ğŸš€ Iniciando prueba completa de integraciÃ³n...")
    
    # Limpiar mÃ©tricas antes de empezar
    metrics_service = get_metrics_service()
    if metrics_service:
        metrics_service.clear_metrics()
        metrics_service.clear_alerts()
        logger.info("ğŸ§¹ MÃ©tricas y alertas limpiadas")
    
    # Test 1: Operaciones exitosas
    logger.info("\nğŸ“Š Test 1: Operaciones exitosas")
    try:
        result1 = await test_pipefy_success()
        logger.info(f"âœ… Pipefy: {result1}")
        
        result2 = await test_twilio_success()
        logger.info(f"âœ… Twilio: {result2}")
        
        result3 = test_cnpj_sync_success()
        logger.info(f"âœ… CNPJ: {result3}")
    except Exception as e:
        logger.error(f"âŒ Error inesperado en operaciones exitosas: {e}")
    
    # Test 2: Operaciones con fallos
    logger.info("\nğŸ“Š Test 2: Operaciones con fallos (esperados)")
    
    # Pipefy failure (con retry)
    try:
        await test_pipefy_failure()
    except Exception as e:
        logger.info(f"ğŸ”¥ Pipefy failure (esperado): {e}")
    
    # CrewAI timeout (con retry)
    try:
        await test_crewai_timeout()
    except Exception as e:
        logger.info(f"â±ï¸ CrewAI timeout (esperado): {e}")
    
    # Supabase connection error (con retry)
    try:
        await test_supabase_failure()
    except Exception as e:
        logger.info(f"ğŸ’¥ Supabase failure (esperado): {e}")
    
    # Test 3: Carga de trabajo mixta
    logger.info("\nğŸ“Š Test 3: Carga de trabajo mixta")
    
    # Ejecutar mÃºltiples operaciones en paralelo
    tasks = []
    
    # Agregar operaciones exitosas
    for i in range(3):
        tasks.append(test_pipefy_success())
        tasks.append(test_twilio_success())
    
    # Agregar algunas operaciones que fallan
    tasks.append(test_pipefy_failure())
    tasks.append(test_crewai_timeout())
    
    # Ejecutar todas en paralelo
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    successes = sum(1 for r in results if not isinstance(r, Exception))
    failures = len(results) - successes
    
    logger.info(f"ğŸ“ˆ Operaciones paralelas: {successes} Ã©xitos, {failures} fallos")
    
    # Test 4: Operaciones sÃ­ncronas adicionales
    logger.info("\nğŸ“Š Test 4: Operaciones sÃ­ncronas adicionales")
    for i in range(3):
        try:
            result = test_cnpj_sync_success()
            logger.info(f"âœ… CNPJ sync #{i+1}: {result['company']}")
        except Exception as e:
            logger.error(f"âŒ CNPJ sync #{i+1} error: {e}")
    
    # Mostrar mÃ©tricas finales integradas
    await show_final_metrics()

async def show_final_metrics():
    """Muestra las mÃ©tricas finales capturadas automÃ¡ticamente."""
    logger.info("\nğŸ“ˆ MÃ‰TRICAS FINALES (Capturadas AutomÃ¡ticamente):")
    
    metrics_service = get_metrics_service()
    if not metrics_service:
        logger.warning("âš ï¸ Servicio de mÃ©tricas no disponible")
        return
    
    all_metrics = metrics_service.get_all_metrics()
    
    # Mostrar mÃ©tricas por servicio
    for service_name, service_metrics in all_metrics["services"].items():
        if service_metrics['total_requests'] > 0:  # Solo mostrar servicios con actividad
            logger.info(f"\nğŸ”§ {service_name.upper()}:")
            logger.info(f"  ğŸ“Š Total requests: {service_metrics['total_requests']}")
            logger.info(f"  âœ… Successful: {service_metrics['successful_requests']}")
            logger.info(f"  âŒ Failed: {service_metrics['failed_requests']}")
            logger.info(f"  â±ï¸ Timeouts: {service_metrics['timeout_requests']}")
            logger.info(f"  ğŸ“ˆ Success rate: {service_metrics['success_rate']:.1f}%")
            logger.info(f"  âš¡ Avg response time: {service_metrics['avg_response_time_seconds']:.3f}s")
            logger.info(f"  ğŸ”„ Consecutive failures: {service_metrics['consecutive_failures']}")
            logger.info(f"  ğŸš¨ Circuit breaker open: {service_metrics['circuit_breaker_open']}")
            
            if service_metrics['last_success']:
                logger.info(f"  âœ… Last success: {service_metrics['last_success']}")
            if service_metrics['last_failure']:
                logger.info(f"  âŒ Last failure: {service_metrics['last_failure']}")
    
    # Mostrar alertas generadas automÃ¡ticamente
    alerts = metrics_service.get_recent_alerts(hours=1)
    if alerts:
        logger.info(f"\nğŸš¨ ALERTAS AUTOMÃTICAS GENERADAS ({len(alerts)}):")
        for alert in alerts:
            logger.info(f"  [{alert['level'].upper()}] {alert['service']}: {alert['message']}")
            if alert['context']:
                logger.info(f"    Context: {alert['context']}")
    else:
        logger.info("\nâœ… No se generaron alertas automÃ¡ticas")
    
    # Mostrar resumen general
    summary = all_metrics["summary"]
    logger.info(f"\nğŸ“‹ RESUMEN GENERAL DEL SISTEMA:")
    logger.info(f"  ğŸ“Š Total requests: {summary['total_requests']}")
    logger.info(f"  âœ… Total successful: {summary['total_successful']}")
    logger.info(f"  âŒ Total failed: {summary['total_failed']}")
    logger.info(f"  ğŸ“ˆ Overall success rate: {summary['overall_success_rate']:.1f}%")
    logger.info(f"  ğŸ”¥ Services with failures: {summary['services_with_failures']}")
    logger.info(f"  ğŸš¨ Services with circuit open: {summary['services_with_circuit_open']}")
    logger.info(f"  ğŸ¢ Total services monitored: {summary['total_services']}")
    
    # Exportar mÃ©tricas a archivo
    export_file = "metrics_export.json"
    metrics_service.export_metrics(export_file)
    logger.info(f"\nğŸ’¾ MÃ©tricas exportadas a: {export_file}")

def main():
    """FunciÃ³n principal."""
    try:
        asyncio.run(run_comprehensive_test())
        logger.info("\nğŸ‰ Prueba completa de integraciÃ³n finalizada exitosamente!")
        logger.info("\nâœ¨ El sistema ahora captura mÃ©tricas automÃ¡ticamente en todas las llamadas a APIs externas")
        logger.info("ğŸ” Revisa el archivo metrics_export.json para ver todas las mÃ©tricas capturadas")
    except Exception as e:
        logger.error(f"ğŸ’¥ Error durante la prueba completa: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main() 