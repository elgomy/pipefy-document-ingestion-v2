#!/usr/bin/env python3
"""
Script de prueba simple para verificar la integraciÃ³n de mÃ©tricas.
"""

import asyncio
import time
import logging
import sys
import os
from pathlib import Path

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

# Agregar src al path de Python
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

# Importar directamente las mÃ©tricas
from services.metrics_service import MetricsService, ServiceType

# Crear instancia global de mÃ©tricas
metrics_service = MetricsService()

# FunciÃ³n simple para probar mÃ©tricas
def test_metrics():
    """Prueba bÃ¡sica del sistema de mÃ©tricas."""
    logger.info("ğŸš€ Iniciando prueba simple de mÃ©tricas...")
    
    # Limpiar mÃ©tricas
    metrics_service.clear_metrics()
    
    # Simular algunas operaciones
    logger.info("ğŸ“Š Registrando operaciones de prueba...")
    
    # Ã‰xitos
    metrics_service.record_request(ServiceType.PIPEFY, True, 0.1, False)
    metrics_service.record_request(ServiceType.TWILIO, True, 0.2, False)
    metrics_service.record_request(ServiceType.CNPJ, True, 0.15, False)
    
    # Fallos
    metrics_service.record_request(ServiceType.PIPEFY, False, 0.5, False, "API Error")
    metrics_service.record_request(ServiceType.CREWAI, False, 30.0, True, "Timeout")
    metrics_service.record_request(ServiceType.SUPABASE, False, 0.1, False, "Connection Error")
    
    # MÃ¡s Ã©xitos para generar estadÃ­sticas
    for i in range(5):
        metrics_service.record_request(ServiceType.PIPEFY, True, 0.1 + i * 0.02, False)
    
    # Mostrar mÃ©tricas
    logger.info("\nğŸ“ˆ MÃ‰TRICAS REGISTRADAS:")
    all_metrics = metrics_service.get_all_metrics()
    
    for service_name, service_metrics in all_metrics["services"].items():
        logger.info(f"\nğŸ”§ {service_name.upper()}:")
        logger.info(f"  ğŸ“Š Total requests: {service_metrics['total_requests']}")
        logger.info(f"  âœ… Successful: {service_metrics['successful_requests']}")
        logger.info(f"  âŒ Failed: {service_metrics['failed_requests']}")
        logger.info(f"  â±ï¸ Timeouts: {service_metrics['timeout_requests']}")
        logger.info(f"  ğŸ“ˆ Success rate: {service_metrics['success_rate']:.1f}%")
        logger.info(f"  âš¡ Avg response time: {service_metrics['avg_response_time_seconds']:.3f}s")
        logger.info(f"  ğŸ”„ Consecutive failures: {service_metrics['consecutive_failures']}")
        logger.info(f"  ğŸš¨ Circuit breaker open: {service_metrics['circuit_breaker_open']}")
    
    # Mostrar alertas
    alerts = metrics_service.get_recent_alerts(hours=1)
    if alerts:
        logger.info(f"\nğŸš¨ ALERTAS GENERADAS ({len(alerts)}):")
        for alert in alerts:
            logger.info(f"  [{alert['level'].upper()}] {alert['service']}: {alert['message']}")
    else:
        logger.info("\nâœ… No se generaron alertas")
    
    # Mostrar resumen
    summary = all_metrics["summary"]
    logger.info(f"\nğŸ“‹ RESUMEN GENERAL:")
    logger.info(f"  ğŸ“Š Total requests: {summary['total_requests']}")
    logger.info(f"  âœ… Total successful: {summary['total_successful']}")
    logger.info(f"  âŒ Total failed: {summary['total_failed']}")
    logger.info(f"  ğŸ“ˆ Overall success rate: {summary['overall_success_rate']:.1f}%")
    logger.info(f"  ğŸ”¥ Services with failures: {summary['services_with_failures']}")
    logger.info(f"  ğŸš¨ Services with circuit open: {summary['services_with_circuit_open']}")
    
    logger.info("\nğŸ‰ Prueba de mÃ©tricas completada!")

if __name__ == "__main__":
    try:
        test_metrics()
    except Exception as e:
        logger.error(f"ğŸ’¥ Error durante la prueba: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1) 